{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(language):\n",
    "  # Set the directory path\n",
    "  directory_path = 'C:\\Research\\labeled_features\\{}'.format(language)\n",
    "\n",
    "  # Create an empty list to store the dataframes\n",
    "  dataframes = []\n",
    "\n",
    "  # Iterate through all the files in the directory\n",
    "  for file in os.listdir(directory_path):\n",
    "    # Check if the file is a CSV file\n",
    "    if file.endswith('.csv'):\n",
    "      # Read the CSV file into a Pandas dataframe\n",
    "      df = pd.read_csv(os.path.join(directory_path, file))\n",
    "\n",
    "      # Drop rows where the 'label' column is not what was expected\n",
    "      df = df[df['label'].isin([0, 1, 2, 3])]\n",
    "\n",
    "      # Append the dataframe to the list\n",
    "      dataframes.append(df)\n",
    "\n",
    "  # Concatenate all the dataframes into a single dataframe\n",
    "  df_all = pd.concat(dataframes)\n",
    "\n",
    "  # Model was predicting negative values, so I had to remove the negative values\n",
    "  df_all = df_all[df_all['label'].isin([0, 1, 2, 3])]\n",
    "\n",
    "  # Add column with the index of each row, reset the indices\n",
    "  df_all = df_all.reset_index(drop=True)\n",
    "\n",
    "  # Create a list where at each index is the sample_id of that row\n",
    "  sample_ids = df_all['sample_id'].tolist()\n",
    "\n",
    "  # Drop the 'sample_id' column\n",
    "  return pd.DataFrame(df_all.drop(columns=['sample_id'])), sample_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframes_knn(language):\n",
    "  # Set the directory path\n",
    "  directory_path = 'C:\\Research\\labeled_features\\{}'.format(language)\n",
    "\n",
    "  # Create an empty list to store the train dataframes\n",
    "  train_set = []\n",
    "\n",
    "  # Create an empty list to store the test dataframes\n",
    "  test = []\n",
    "\n",
    "  # Number of files in directory\n",
    "  num_files = len(os.listdir(directory_path))\n",
    "\n",
    "  # Index of the last file that is lower than 80% of the total number of files rounded down\n",
    "  first_train_file = int(np.floor(num_files * 0.8))\n",
    "\n",
    "  # Iterate through all the files in the directory\n",
    "  for i, file in enumerate(os.listdir(directory_path)):\n",
    "    # Check if the file is a CSV file\n",
    "    if file.endswith('.csv'):\n",
    "      # Read the CSV file into a Pandas dataframe\n",
    "      df = pd.read_csv(os.path.join(directory_path, file))\n",
    "\n",
    "      # Drop rows where the 'label' column is not what was expected\n",
    "      df = df[df['label'].isin([0, 1, 2, 3])]\n",
    "\n",
    "      # Append the dataframe to the corresponding list\n",
    "      if i < first_train_file:\n",
    "        train_set.append(df)\n",
    "      else:\n",
    "        test.append(df)\n",
    "\n",
    "  # Concatenate the test dataframes into a single dataframe\n",
    "  df_test = pd.concat(test).drop(columns=['sample_id'])\n",
    "\n",
    "  # Concatenate the train dataframes into a single dataframe\n",
    "  df_train = pd.concat(train_set).drop(columns=['sample_id'])\n",
    "\n",
    "  # Split the train dataframe into X and y\n",
    "  X = df_train.iloc[:, :-1]\n",
    "  y = df_train['label']\n",
    "\n",
    "  # Split the test dataframe into X and y\n",
    "  X_test = df_test.iloc[:, :-1]\n",
    "  y_test = df_test['label']\n",
    "\n",
    "  return X, y, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframes_divided(language):\n",
    "  # Set the directory path\n",
    "  directory_path = 'C:\\Research\\labeled_features\\{}'.format(language)\n",
    "\n",
    "  # Create an empty list to store the train dataframes\n",
    "  train_set = []\n",
    "\n",
    "  # Create an empty list to store the test dataframes\n",
    "  test = []\n",
    "\n",
    "  # Number of files in directory\n",
    "  num_files = len(os.listdir(directory_path))\n",
    "\n",
    "  # Index of the last file that is lower than 80% of the total number of files rounded down\n",
    "  first_train_file = int(np.floor(num_files * 0.8))\n",
    "\n",
    "  # Iterate through all the files in the directory\n",
    "  for i, file in enumerate(os.listdir(directory_path)):\n",
    "    # Check if the file is a CSV file\n",
    "    if file.endswith('.csv'):\n",
    "      # Read the CSV file into a Pandas dataframe\n",
    "      df = pd.read_csv(os.path.join(directory_path, file))\n",
    "\n",
    "      # Drop rows where the 'label' column is not what was expected\n",
    "      df = df[df['label'].isin([0, 1, 2, 3])]\n",
    "\n",
    "      # Append the dataframe to the corresponding list\n",
    "      if i <= first_train_file:\n",
    "        train_set.append(df)\n",
    "      else:\n",
    "        test.append(df)\n",
    "\n",
    "  # Concatenate the test dataframes into a single dataframe\n",
    "  df_test = pd.concat(test).drop(columns=['sample_id'])\n",
    "\n",
    "  # Concatenate the train dataframes into a single dataframe\n",
    "  df_train = pd.concat(train_set).drop(columns=['sample_id'])\n",
    "\n",
    "  return df_train, df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, X_test):\n",
    "    # Make predictions on the testing data\n",
    "    unrounded_predictions = model.predict(X_test)\n",
    "\n",
    "    # Round the predictions to the nearest integer\n",
    "    predictions = np.round(unrounded_predictions)\n",
    "\n",
    "    # Make all negative predictions 0\n",
    "    predictions[predictions < 0] = 0\n",
    "\n",
    "    return unrounded_predictions, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conf_matrix(conf_matrix, language_model, language_test):\n",
    "    # Convert the confusion matrix to a Pandas dataframe\n",
    "    cm_df = pd.DataFrame(conf_matrix)#, index=['true 0', 'true 1', 'true 2', 'true 3'], columns=['pred 0', 'pred 1', 'pred 2', 'pred 3'])\n",
    "\n",
    "    # Create a heatmap of the confusion matrix\n",
    "    fig = px.imshow(cm_df, title='Confusion Matrix', text_auto=True)\n",
    "    fig.update_layout(title='Confusion Matrix Predicted with {} model, on {} data'.format(language_model, language_test), xaxis_title='Predicted', yaxis_title='Expected')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlations(df_all, language_model, language_test):\n",
    "    # Calculate the correlations between the columns and the label\n",
    "    correlations = df_all.corr()['label'].iloc[:-1]\n",
    "\n",
    "    # Create a scatter plot of the correlations\n",
    "    fig = px.scatter(x=correlations.index, y=correlations, title='Correlations')\n",
    "    fig.update_layout(title='Correlations between features and label, predicted with {} model, on {} data'.format(language_model, language_test), xaxis_title='Feature', yaxis_title='Correlation')\n",
    "    fig.show()\n",
    "\n",
    "    # Create a dataframe from the correlations\n",
    "    correlations_df = pd.DataFrame(correlations)\n",
    "\n",
    "    # correlations_df.style.background_gradient(cmap ='viridis')\\\n",
    "    # .set_properties(**{'font-size': '20px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlation(unrounded_predictions, y_test):\n",
    "    expected_predicted_df = pd.DataFrame()\n",
    "    expected_predicted_df['expected'] = y_test\n",
    "    expected_predicted_df['predicted'] = unrounded_predictions\n",
    "    corr = expected_predicted_df.corr()['predicted'].iloc[0]\n",
    "    return expected_predicted_df, corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_expected_vs_predicted(expected_predicted_df, language_model, language_test):\n",
    "  fig = go.Figure()\n",
    "\n",
    "  labels = expected_predicted_df['expected'].unique()\n",
    "\n",
    "  for label in labels:\n",
    "    fig.add_trace(go.Violin(\n",
    "    x=expected_predicted_df['expected'][expected_predicted_df['expected'] == label],\n",
    "    y=expected_predicted_df['predicted'][expected_predicted_df['expected'] == label],\n",
    "    name=label,\n",
    "    box_visible=True,\n",
    "    meanline_visible=False,\n",
    "    ))\n",
    "\n",
    "  fig.update_layout(title='Expected vs Predicted, predicted with {} model, on {} data'.format(language_model, language_test), xaxis_title='Expected', yaxis_title='Predicted')\n",
    "  fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_class_accuray(conf_matrix):\n",
    "    # Calculate for each of the expected classes, what percentage of the predictions are correct\n",
    "    correct_percentage_0 = conf_matrix[0,0] / np.sum(conf_matrix[0,:]) # 0\n",
    "    correct_percentage_1 = conf_matrix[1,1] / np.sum(conf_matrix[1,:]) # 1\n",
    "    correct_percentage_2 = conf_matrix[2,2] / np.sum(conf_matrix[2,:]) # 2\n",
    "    correct_percentage_3 = conf_matrix[3,3] / np.sum(conf_matrix[3,:]) # 3\n",
    "\n",
    "    # Print the percentages\n",
    "    print(\"Percentage of correct predictions for class 0: {:.2f}%\".format(correct_percentage_0 * 100))\n",
    "    print(\"Percentage of correct predictions for class 1: {:.2f}%\".format(correct_percentage_1 * 100))\n",
    "    print(\"Percentage of correct predictions for class 2: {:.2f}%\".format(correct_percentage_2 * 100))\n",
    "    print(\"Percentage of correct predictions for class 3: {:.2f}%\".format(correct_percentage_3 * 100))\n",
    "\n",
    "    # Calculate the acuracy of predicting 0 or 1\n",
    "    correct_percentage_0_1 = (conf_matrix[0,0] + conf_matrix[1,1]) / np.sum(conf_matrix[0:2,:]) # 0 or 1\n",
    "\n",
    "    # Calculate the acuracy of predicting 2 or 3\n",
    "    correct_percentage_2_3 = (conf_matrix[2,2] + conf_matrix[3,3]) / np.sum(conf_matrix[2:4,:]) # 2 or 3\n",
    "\n",
    "    # Print the percentages\n",
    "    print(\"Percentage of correct predictions for class 0 or 1: {:.2f}%\".format(correct_percentage_0_1 * 100))\n",
    "    print(\"Percentage of correct predictions for class 2 or 3: {:.2f}%\".format(correct_percentage_2_3 * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_rows(df, label, times):\n",
    "    # Get the rows where the label is 'label\n",
    "    df_label = df[df['label'] == label]\n",
    "\n",
    "    # Duplicate the rows\n",
    "    df_label = pd.concat([df_label] * times)\n",
    "\n",
    "    # Add the duplicated rows to the original dataframe\n",
    "    df = pd.concat([df, df_label])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standarize_data(df):\n",
    "    scaler = StandardScaler()\n",
    "    df_standardized = scaler.fit_transform(df.iloc[:, :-1])\n",
    "    df_standardized = pd.DataFrame(df_standardized, columns=df.columns[:-1].tolist())\n",
    "    # Check for duplicate column names\n",
    "    if df.columns[-1] in df_standardized.columns:\n",
    "        df_standardized.columns = [str(col) + '_1' if col == df.columns[-1] else col for col in df_standardized.columns]\n",
    "    df_standardized[df.columns[-1]] = df[df.columns[-1]]\n",
    "    \n",
    "    return df_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "train_df, test_df = get_dataframes_divided(\"EN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print duplicate columns of a dataframe\n",
    "def print_duplicate_columns(df):\n",
    "    duplicate_columns = df.columns[df.columns.duplicated()]\n",
    "    print(duplicate_columns)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "train_df, test_df = get_dataframes_divided(\"EN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique values of the labels\n",
    "unique_labels = np.unique(train_df['label'])\n",
    "print(unique_labels)\n",
    "unique_labels = np.unique(test_df['label'])\n",
    "print(unique_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarize the data\n",
    "train_df = standarize_data(train_df.reset_index(drop=True))\n",
    "test_df = standarize_data(test_df.reset_index(drop=True))\n",
    "\n",
    "X_train_EN = train_df.iloc[:, :-1]\n",
    "y_train_EN = train_df.iloc[:, -1]\n",
    "X_test_EN = test_df.iloc[:, :-1]\n",
    "y_test_EN = test_df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_EN.shape)\n",
    "print(y_train_EN.shape)\n",
    "print(X_test_EN.shape)\n",
    "print(y_test_EN.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "train_df, test_df = get_dataframes_divided(\"ES\")\n",
    "\n",
    "# Standarize the data\n",
    "train_df = standarize_data(train_df.reset_index(drop=True))\n",
    "test_df = standarize_data(test_df.reset_index(drop=True))\n",
    "\n",
    "X_train_ES = train_df.iloc[:, :-1]\n",
    "y_train_ES = train_df.iloc[:, -1]\n",
    "X_test_ES = test_df.iloc[:, :-1]\n",
    "y_test_ES = test_df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_ES.shape)\n",
    "print(y_train_ES.shape)\n",
    "print(X_test_ES.shape)\n",
    "print(y_test_ES.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = train_df.corr()\n",
    "correlations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the logs use: \n",
    "tensorboard --logdir \"C:/Users/carlo/Research/reduction-detection/logs/fit/\" and look at http://localhost:6006\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model = 'EN' # 'EN' or 'ES'\n",
    "language_test = 'EN' # 'EN' or 'ES'\n",
    "\n",
    "model_ID = language_model + \"_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Create a TensorBoard object with a log directory\n",
    "logdir = \"C:/Users/carlo/Research/reduction-detection/logs/fit/{model_ID}\".format(model_ID=model_ID)\n",
    "tensorboard_callback = TensorBoard(log_dir=logdir)\n",
    "\n",
    "# Create the EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feed forward model that works with a classification problem\n",
    "model_ffnn_EN = Sequential()\n",
    "\n",
    "# Add the input layer\n",
    "model_ffnn_EN.add(Dense(80, input_dim=80, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "\n",
    "# Add the hidden layer\n",
    "model_ffnn_EN.add(Dense(80, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "\n",
    "model_ffnn_EN.add(Dropout(0.2))\n",
    "\n",
    "# Add the hidden layer\n",
    "model_ffnn_EN.add(Dense(122, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "\n",
    "model_ffnn_EN.add(Dropout(0.2))\n",
    "\n",
    "# # Add the hidden layer\n",
    "# model_ffnn_EN.add(Dense(122, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "\n",
    "# model_ffnn_EN.add(Dropout(0.2))\n",
    "\n",
    "# # Add the hidden layer\n",
    "# model_ffnn_EN.add(Dense(122, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "\n",
    "# model_ffnn_EN.add(Dropout(0.2))\n",
    "\n",
    "# # Add the hidden layer\n",
    "# model_ffnn_EN.add(Dense(122, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "\n",
    "# model_ffnn_EN.add(Dropout(0.2))\n",
    "\n",
    "# # Add the hidden layer\n",
    "# model_ffnn_EN.add(Dense(122, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "\n",
    "# model_ffnn_EN.add(Dropout(0.2))\n",
    "\n",
    "# Add the output layer\n",
    "model_ffnn_EN.add(Dense(1, activation='relu'))\n",
    "\n",
    "# Compile the model\n",
    "model_ffnn_EN.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 1000\n",
    "model_ffnn_EN.fit(X_train_EN, y_train_EN, batch_size=batch_size, epochs=epochs, validation_data=(X_test_EN, y_test_EN), callbacks=[early_stopping, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique values of the labels\n",
    "unique_labels = np.unique(test_df['label'])\n",
    "\n",
    "unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions\n",
    "unrounded_predictions, predictions = get_predictions(model_ffnn_EN, X_test_EN)\n",
    "\n",
    "# Round predictions greater than 3 to 3\n",
    "predictions = np.where(predictions > 3, 3, predictions)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_EN, predictions)\n",
    "\n",
    "print_class_accuray(conf_matrix)\n",
    "\n",
    "# Get the accuracy\n",
    "accuracy = accuracy_score(y_test_EN, predictions)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy*100))\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_acc = model_ffnn_EN.evaluate(X_test_EN, y_test_EN, verbose=0)\n",
    "\n",
    "plot_conf_matrix(conf_matrix, language_model, language_test)\n",
    "\n",
    "expected_predicted_df, corr = compute_correlation(unrounded_predictions, y_test_EN)\n",
    "print(\"Correlation between expected and predicted: {}\".format(corr))\n",
    "\n",
    "plot_expected_vs_predicted(expected_predicted_df, language_model, language_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ffnn_EN.save('models/EN/ffnn_acc_{}_val_loss_{}_{}_{}'.format(accuracy, test_loss, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"), model_ID))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ffnn_EN = keras.models.load_model('models/EN/ffnn_20230131-113400')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions\n",
    "unrounded_predictions, predictions = get_predictions(model_ffnn_EN, X_test_EN)\n",
    "\n",
    "# Round predictions greater than 3 to 3\n",
    "predictions = np.where(predictions > 3, 3, predictions)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_EN, predictions)\n",
    "\n",
    "print_class_accuray(conf_matrix)\n",
    "\n",
    "# Get the accuracy\n",
    "accuracy = accuracy_score(y_test_EN, predictions)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy*100))\n",
    "\n",
    "plot_conf_matrix(conf_matrix, language_model, language_test)\n",
    "\n",
    "expected_predicted_df, corr = compute_correlation(unrounded_predictions, y_test_EN)\n",
    "print(\"Correlation between expected and predicted: {}\".format(corr))\n",
    "\n",
    "plot_expected_vs_predicted(expected_predicted_df, language_model, language_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model = 'ES' # 'EN' or 'ES'\n",
    "language_test = 'ES' # 'EN' or 'ES'\n",
    "\n",
    "model_ID = language_model + \"_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Create a TensorBoard object with a log directory\n",
    "logdir = \"C:/Users/carlo/Research/reduction-detection/logs/fit/{model_ID}\".format(model_ID=model_ID)\n",
    "tensorboard_callback = TensorBoard(log_dir=logdir)\n",
    "\n",
    "# Create the EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ffnn_ES = Sequential()\n",
    "\n",
    "# Add the input layer\n",
    "model_ffnn_ES.add(Dense(61, input_dim=80, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "\n",
    "# Add the hidden layer\n",
    "model_ffnn_ES.add(Dense(61, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "\n",
    "model_ffnn_ES.add(Dropout(0.2))\n",
    "\n",
    "# Add the hidden layer\n",
    "model_ffnn_ES.add(Dense(122, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "\n",
    "model_ffnn_ES.add(Dropout(0.2))\n",
    "\n",
    "# Add the hidden layer\n",
    "model_ffnn_ES.add(Dense(122, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "\n",
    "model_ffnn_ES.add(Dropout(0.2))\n",
    "\n",
    "# Add the hidden layer\n",
    "model_ffnn_ES.add(Dense(122, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "\n",
    "model_ffnn_ES.add(Dropout(0.2))\n",
    "\n",
    "# Add the hidden layer\n",
    "model_ffnn_ES.add(Dense(122, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "\n",
    "model_ffnn_ES.add(Dropout(0.2))\n",
    "\n",
    "# Add the hidden layer\n",
    "model_ffnn_ES.add(Dense(122, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "\n",
    "model_ffnn_ES.add(Dropout(0.2))\n",
    "\n",
    "# Add the output layer\n",
    "model_ffnn_ES.add(Dense(1, activation='relu'))\n",
    "\n",
    "# Compile the model\n",
    "model_ffnn_ES.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 1000\n",
    "model_ffnn_ES.fit(X_train_ES, y_train_ES, batch_size=batch_size, epochs=epochs, validation_data=(X_test_ES, y_test_ES), callbacks=[early_stopping, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions\n",
    "unrounded_predictions, predictions = get_predictions(model_ffnn_ES, X_test_ES)\n",
    "\n",
    "# Round predictions greater than 3 to 3\n",
    "predictions = np.where(predictions > 3, 3, predictions)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_ES, predictions)\n",
    "\n",
    "print_class_accuray(conf_matrix)\n",
    "\n",
    "# Get the accuracy\n",
    "accuracy = accuracy_score(y_test_ES, predictions)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_acc = model_ffnn_ES.evaluate(X_test_ES, y_test_ES, verbose=0)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy*100))\n",
    "\n",
    "plot_conf_matrix(conf_matrix, language_model, language_test)\n",
    "\n",
    "expected_predicted_df, corr = compute_correlation(unrounded_predictions, y_test_ES)\n",
    "print(\"Correlation between expected and predicted: {}\".format(corr))\n",
    "\n",
    "plot_expected_vs_predicted(expected_predicted_df, language_model, language_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ffnn_ES.save('models/ES/ffnn_acc_{}_val_loss_{}_{}_{}'.format(accuracy, test_loss, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"), model_ID))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ffnn_ES = keras.models.load_model('models/ES/ffnn_20230131-113400')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions\n",
    "unrounded_predictions, predictions = get_predictions(model_ffnn_ES, X_test_ES)\n",
    "\n",
    "# Round predictions greater than 3 to 3\n",
    "predictions = np.where(predictions > 3, 3, predictions)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_ES, predictions)\n",
    "\n",
    "print_class_accuray(conf_matrix)\n",
    "\n",
    "# Get the accuracy\n",
    "accuracy = accuracy_score(y_test_EN, predictions)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy*100))\n",
    "\n",
    "plot_conf_matrix(conf_matrix, language_model, language_test)\n",
    "\n",
    "expected_predicted_df, corr = compute_correlation(unrounded_predictions, y_test_EN)\n",
    "print(\"Correlation between expected and predicted: {}\".format(corr))\n",
    "\n",
    "plot_expected_vs_predicted(expected_predicted_df, language_model, language_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
