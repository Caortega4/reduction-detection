{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(language):\n",
    "  # Set the directory path\n",
    "  directory_path = 'C:\\Research\\labeled_features\\{}'.format(language)\n",
    "\n",
    "  # Create an empty list to store the dataframes\n",
    "  dataframes = []\n",
    "\n",
    "  # Iterate through all the files in the directory\n",
    "  for file in os.listdir(directory_path):\n",
    "    # Check if the file is a CSV file\n",
    "    if file.endswith('.csv'):\n",
    "      # Read the CSV file into a Pandas dataframe\n",
    "      df = pd.read_csv(os.path.join(directory_path, file))\n",
    "\n",
    "      # Drop rows where the 'label' column is not what was expected\n",
    "      df = df[df['label'].isin([0, 1, 2, 3])]\n",
    "\n",
    "      # Append the dataframe to the list\n",
    "      dataframes.append(df)\n",
    "\n",
    "  # Concatenate all the dataframes into a single dataframe\n",
    "  df_all = pd.concat(dataframes)\n",
    "\n",
    "  # Model was predicting negative values, so I had to remove the negative values\n",
    "  df_all = df_all[df_all['label'].isin([0, 1, 2, 3])]\n",
    "\n",
    "  # Add column with the index of each row, reset the indices\n",
    "  df_all = df_all.reset_index(drop=True)\n",
    "\n",
    "  # Create a list where at each index is the sample_id of that row\n",
    "  sample_ids = df_all['sample_id'].tolist()\n",
    "\n",
    "  # Drop the 'sample_id' column\n",
    "  return pd.DataFrame(df_all.drop(columns=['sample_id'])), sample_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframes_knn(language):\n",
    "  # Set the directory path\n",
    "  directory_path = 'C:\\Research\\labeled_features\\{}'.format(language)\n",
    "\n",
    "  # Create an empty list to store the train dataframes\n",
    "  train_set = []\n",
    "\n",
    "  # Create an empty list to store the test dataframes\n",
    "  test = []\n",
    "\n",
    "  # Number of files in directory\n",
    "  num_files = len(os.listdir(directory_path))\n",
    "\n",
    "  # Index of the last file that is lower than 80% of the total number of files rounded down\n",
    "  first_train_file = int(np.floor(num_files * 0.8))\n",
    "\n",
    "  # Iterate through all the files in the directory\n",
    "  for i, file in enumerate(os.listdir(directory_path)):\n",
    "    # Check if the file is a CSV file\n",
    "    if file.endswith('.csv'):\n",
    "      # Read the CSV file into a Pandas dataframe\n",
    "      df = pd.read_csv(os.path.join(directory_path, file))\n",
    "\n",
    "      # Drop rows where the 'label' column is not what was expected\n",
    "      df = df[df['label'].isin([0, 1, 2, 3])]\n",
    "\n",
    "      # Append the dataframe to the corresponding list\n",
    "      if i < first_train_file:\n",
    "        train_set.append(df)\n",
    "      else:\n",
    "        test.append(df)\n",
    "\n",
    "  # Concatenate the test dataframes into a single dataframe\n",
    "  df_test = pd.concat(test).drop(columns=['sample_id'])\n",
    "\n",
    "  # Concatenate the train dataframes into a single dataframe\n",
    "  df_train = pd.concat(train_set).drop(columns=['sample_id'])\n",
    "\n",
    "  # Split the train dataframe into X and y\n",
    "  X = df_train.iloc[:, :-1]\n",
    "  y = df_train['label']\n",
    "\n",
    "  # Split the test dataframe into X and y\n",
    "  X_test = df_test.iloc[:, :-1]\n",
    "  y_test = df_test['label']\n",
    "\n",
    "  return X, y, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframes_divided(language):\n",
    "  # Set the directory path\n",
    "  directory_path = 'C:\\Research\\labeled_features\\{}'.format(language)\n",
    "\n",
    "  # Create an empty list to store the train dataframes\n",
    "  train_set = []\n",
    "\n",
    "  # Create an empty list to store the test dataframes\n",
    "  test = []\n",
    "\n",
    "  # Number of files in directory\n",
    "  num_files = len(os.listdir(directory_path))\n",
    "\n",
    "  # Index of the last file that is lower than 80% of the total number of files rounded down\n",
    "  first_train_file = int(np.floor(num_files * 0.8))\n",
    "\n",
    "  # Iterate through all the files in the directory\n",
    "  for i, file in enumerate(os.listdir(directory_path)):\n",
    "    # Check if the file is a CSV file\n",
    "    if file.endswith('.csv'):\n",
    "      # Read the CSV file into a Pandas dataframe\n",
    "      df = pd.read_csv(os.path.join(directory_path, file))\n",
    "\n",
    "      # Drop rows where the 'label' column is not what was expected\n",
    "      df = df[df['label'].isin([0, 1, 2, 3])]\n",
    "\n",
    "      # Append the dataframe to the corresponding list\n",
    "      if i < first_train_file:\n",
    "        train_set.append(df)\n",
    "      else:\n",
    "        test.append(df)\n",
    "\n",
    "  # Concatenate the test dataframes into a single dataframe\n",
    "  df_test = pd.concat(test).drop(columns=['sample_id'])\n",
    "\n",
    "  # Concatenate the train dataframes into a single dataframe\n",
    "  df_train = pd.concat(train_set).drop(columns=['sample_id'])\n",
    "\n",
    "  # Split the train dataframe into X and y\n",
    "  X = df_train.iloc[:, :-1]\n",
    "  y = df_train['label']\n",
    "\n",
    "  # Split the test dataframe into X and y\n",
    "  X_test = df_test.iloc[:, :-1]\n",
    "  y_test = df_test['label']\n",
    "\n",
    "  return X, y, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(df_all):\n",
    "    # Split the data into features and target\n",
    "    X = df_all.iloc[:, :-1]\n",
    "    df_all_backup = df_all\n",
    "    y = df_all['label']\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create a linear regression model\n",
    "    model = LinearRegression()\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return model, X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, X_test):\n",
    "    # Make predictions on the testing data\n",
    "    unrounded_predictions = model.predict(X_test)\n",
    "\n",
    "    # Round the predictions to the nearest integer\n",
    "    predictions = np.round(unrounded_predictions)\n",
    "\n",
    "    # Make all negative predictions 0\n",
    "    predictions[predictions < 0] = 0\n",
    "\n",
    "    return unrounded_predictions, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conf_matrix(conf_matrix, language_model, language_test):\n",
    "    # Convert the confusion matrix to a Pandas dataframe\n",
    "    cm_df = pd.DataFrame(conf_matrix)#, index=['true 0', 'true 1', 'true 2', 'true 3'], columns=['pred 0', 'pred 1', 'pred 2', 'pred 3'])\n",
    "\n",
    "    # Create a heatmap of the confusion matrix\n",
    "    fig = px.imshow(cm_df, title='Confusion Matrix', text_auto=True)\n",
    "    fig.update_layout(title='Confusion Matrix Predicted with {} model, on {} data'.format(language_model, language_test), xaxis_title='Predicted', yaxis_title='Expected')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlations(df_all, language_model):\n",
    "    # Calculate the correlations between the columns and the label\n",
    "    correlations = df_all.corr()['label'].iloc[:-1]\n",
    "\n",
    "    # Create a scatter plot of the correlations\n",
    "    fig = px.scatter(x=correlations.index, y=correlations, title='Correlations')\n",
    "    fig.update_layout(title='Correlations between features and label {}'.format(language_model), xaxis_title='Feature', yaxis_title='Correlation', yaxis_range=[-0.15, 0.15])\n",
    "    fig.show()\n",
    "\n",
    "    # Create a dataframe from the correlations\n",
    "    correlations_df = pd.DataFrame(correlations)\n",
    "\n",
    "    # correlations_df.style.background_gradient(cmap ='viridis')\\\n",
    "    # .set_properties(**{'font-size': '20px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlation(unrounded_predictions, y_test):\n",
    "    expected_predicted_df = pd.DataFrame()\n",
    "    expected_predicted_df['expected'] = y_test\n",
    "    expected_predicted_df['predicted'] = unrounded_predictions\n",
    "    corr = expected_predicted_df.corr()['predicted'].iloc[0]\n",
    "    return expected_predicted_df, corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_expected_vs_predicted(expected_predicted_df, language_model, language_test):\n",
    "  fig = go.Figure()\n",
    "\n",
    "  labels = expected_predicted_df['expected'].unique()\n",
    "\n",
    "  for label in labels:\n",
    "    fig.add_trace(go.Violin(\n",
    "    x=expected_predicted_df['expected'][expected_predicted_df['expected'] == label],\n",
    "    y=expected_predicted_df['predicted'][expected_predicted_df['expected'] == label],\n",
    "    name=label,\n",
    "    box_visible=True,\n",
    "    meanline_visible=False,\n",
    "    ))\n",
    "\n",
    "  fig.update_layout(title='Expected vs Predicted, predicted with {} model, on {} data'.format(language_model, language_test), xaxis_title='Expected', yaxis_title='Predicted')\n",
    "  fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_coefficients(model, X, corr, language):\n",
    "\n",
    "    #Switch case based on the type of the model\n",
    "    if type(model) == LinearRegression:\n",
    "        model_type = 'linear_regression'\n",
    "    # elif type(model) == sklearn.neighbors._classification.KNeighborsClassifier:\n",
    "    #     model_type = 'knn_classifier'\n",
    "        \n",
    "\n",
    "    # Create a dataframe with the the coefficients and the intercept\n",
    "    coefficients_df = pd.DataFrame()\n",
    "    coefficients_df['feature'] = X.columns\n",
    "    coefficients_df['coefficient'] = model.coef_\n",
    "\n",
    "    # Add row in column 'feature' for the intercept and set the value to 'intercept'\n",
    "    coefficients_df.loc[-1] = ['intercept', model.intercept_]\n",
    "\n",
    "    # Add row for the correlation between the label and the features\n",
    "    coefficients_df.loc[-2] = ['correlation', corr]\n",
    "\n",
    "    filename = 'C:\\Research\\Results\\{lang}\\{model_type}coefficients_{date}_corr_{corr}.csv'.format(model_type=model_type, lang=language, date=datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'), corr=corr)\n",
    "\n",
    "    # Write the dataframe to a CSV file including the date and time in the filename, if the file already exists, throw an error\n",
    "    coefficients_df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_class_accuray(conf_matrix):\n",
    "    # Calculate for each of the expected classes, what percentage of the predictions are correct\n",
    "    correct_percentage_0 = conf_matrix[0,0] / np.sum(conf_matrix[0,:]) # 0\n",
    "    correct_percentage_1 = conf_matrix[1,1] / np.sum(conf_matrix[1,:]) # 1\n",
    "    correct_percentage_2 = conf_matrix[2,2] / np.sum(conf_matrix[2,:]) # 2\n",
    "    correct_percentage_3 = conf_matrix[3,3] / np.sum(conf_matrix[3,:]) # 3\n",
    "\n",
    "    # Print the percentages\n",
    "    print(\"Percentage of correct predictions for class 0: {:.2f}%\".format(correct_percentage_0 * 100))\n",
    "    print(\"Percentage of correct predictions for class 1: {:.2f}%\".format(correct_percentage_1 * 100))\n",
    "    print(\"Percentage of correct predictions for class 2: {:.2f}%\".format(correct_percentage_2 * 100))\n",
    "    print(\"Percentage of correct predictions for class 3: {:.2f}%\".format(correct_percentage_3 * 100))\n",
    "\n",
    "    # Calculate the acuracy of predicting 0 or 1\n",
    "    correct_percentage_0_1 = (conf_matrix[0,0] + conf_matrix[1,1]) / np.sum(conf_matrix[0:2,:]) # 0 or 1\n",
    "\n",
    "    # Calculate the acuracy of predicting 2 or 3\n",
    "    correct_percentage_2_3 = (conf_matrix[2,2] + conf_matrix[3,3]) / np.sum(conf_matrix[2:4,:]) # 2 or 3\n",
    "\n",
    "    # Print the percentages\n",
    "    print(\"Percentage of correct predictions for class 0 or 1: {:.2f}%\".format(correct_percentage_0_1 * 100))\n",
    "    print(\"Percentage of correct predictions for class 2 or 3: {:.2f}%\".format(correct_percentage_2_3 * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_rows(df, label, times):\n",
    "    # Get the rows where the label is 'label\n",
    "    df_label = df[df['label'] == label]\n",
    "\n",
    "    # Duplicate the rows\n",
    "    df_label = pd.concat([df_label] * times)\n",
    "\n",
    "    # Add the duplicated rows to the original dataframe\n",
    "    df = pd.concat([df, df_label])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a dataframe, return a dataframe with the name of the columns and its correlations with the label,\n",
    "# only if the correlation's absolute value is above the threshold\n",
    "def get_significant_correlations(df, threshold):\n",
    "    # Calculate the correlations between the columns and the label\n",
    "    correlations = df.corr()['label'].iloc[:-1]\n",
    "\n",
    "    # Create a dataframe from the correlations\n",
    "    correlations_df = pd.DataFrame(correlations)\n",
    "\n",
    "    # Rename the column to 'correlation'\n",
    "    correlations_df.rename(columns={'label': 'correlation'}, inplace=True)\n",
    "\n",
    "    # Filter out the correlations that are below the threshold\n",
    "    correlations_df = correlations_df[correlations_df['correlation'].abs() > threshold]\n",
    "\n",
    "    return correlations_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "df_all_ES, sample_ids = get_dataframe(\"ES\")\n",
    "# Create the model\n",
    "model_ES, X_train_ES, y_train_ES, X_test_ES, y_test_ES = create_model(df_all_ES)\n",
    "\n",
    "# Get the data\n",
    "df_all_EN, sample_ids = get_dataframe(\"EN\")\n",
    "\n",
    "# Create the model\n",
    "model_EN, X_train_EN, y_train_EN ,X_test_EN, y_test_EN = create_model(df_all_EN)\n",
    "\n",
    "# Get the data\n",
    "X_train_knn_ES, y_train_knn_ES, X_test_knn_ES, y_test_knn_ES = get_dataframes_knn(\"ES\")\n",
    "\n",
    "# Get the data\n",
    "X_train_knn_EN, y_train_knn_EN, X_test_knn_EN, y_test_knn_EN = get_dataframes_knn(\"EN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object to z-standarize data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Get the data\n",
    "df, sample_ids = get_dataframe(\"ES\")\n",
    "\n",
    "df_all_ES = scaler.fit_transform(df.iloc[:, :-1])\n",
    "df_all_ES = pd.DataFrame(df_all_ES, columns=df.columns[:-1])\n",
    "df_all_ES[df.columns[-1]] = df[df.columns[-1]]\n",
    "\n",
    "# Duplicate rows where the label is 3\n",
    "# df_all_ES = duplicate_rows(df_all_ES, 3, 2)\n",
    "\n",
    "# Create the model\n",
    "model_ES, X_train_ES, y_train_ES, X_test_ES, y_test_ES = create_model(df_all_ES)\n",
    "\n",
    "# Get the data\n",
    "df, sample_ids = get_dataframe(\"EN\")\n",
    "\n",
    "df_all_EN = scaler.fit_transform(df.iloc[:, :-1])\n",
    "df_all_EN = pd.DataFrame(df_all_EN, columns=df.columns[:-1])\n",
    "df_all_EN[df.columns[-1]] = df[df.columns[-1]]\n",
    "\n",
    "# Duplicate rows where the label is 3\n",
    "# df_all_EN = duplicate_rows(df_all_EN, 3, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model = 'ES' # 'EN' or 'ES'\n",
    "language_test = 'ES' # 'EN' or 'ES'\n",
    "\n",
    "plot_correlations(df_all_ES, language_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_corr = get_significant_correlations(df_all_ES, 0.06)\n",
    "significant_corr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model = 'EN' # 'EN' or 'ES'\n",
    "language_test = 'EN' # 'EN' or 'ES'\n",
    "\n",
    "plot_correlations(df_all_EN, language_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_corr = get_significant_correlations(df_all_EN, 0.06)\n",
    "significant_corr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the test set as the most common label\n",
    "y_test_pred_most_common = np.full(y_test_ES.shape, y_train_ES.mode()[0])\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test_ES, y_test_pred_most_common)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy of the most common label model: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the test set as the most common label\n",
    "y_test_pred_most_common = np.full(y_test_EN.shape, y_train_EN.mode()[0])\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test_EN, y_test_pred_most_common)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy of the most common label model: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model = 'ES' # 'EN' or 'ES'\n",
    "language_test = 'ES' # 'EN' or 'ES'\n",
    "\n",
    "# Get the predictions\n",
    "unrounded_predictions, predictions = get_predictions(model_ES, X_test_ES)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_ES, predictions)\n",
    "\n",
    "plot_conf_matrix(conf_matrix, language_model, language_test)\n",
    "\n",
    "print_class_accuray(conf_matrix)\n",
    "\n",
    "# Get the accuracy\n",
    "accuracy = accuracy_score(y_test_ES, predictions)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "expected_predicted_df, corr = compute_correlation(unrounded_predictions, y_test_ES)\n",
    "print(\"Correlation between expected and predicted: {}\".format(corr))\n",
    "\n",
    "plot_expected_vs_predicted(expected_predicted_df, language_model, language_test)\n",
    "\n",
    "\n",
    "# Save the coefficients to a CSV file \"C:\\Research\\Results\\{lang}\\coefficients_{date}_corr_{corr}.csv\"\n",
    "save_coefficients(model_ES, X_test_ES, corr, language_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model = 'EN' # 'EN' or 'ES'\n",
    "language_test = 'EN' # 'EN' or 'ES'\n",
    "\n",
    "# Get the predictions\n",
    "unrounded_predictions, predictions = get_predictions(model_EN, X_test_EN)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_EN, predictions)\n",
    "\n",
    "plot_conf_matrix(conf_matrix, language_model, language_test)\n",
    "\n",
    "print_class_accuray(conf_matrix)\n",
    "\n",
    "# Get the accuracy\n",
    "accuracy = accuracy_score(y_test_EN, predictions)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "expected_predicted_df, corr = compute_correlation(unrounded_predictions, y_test_EN)\n",
    "print(\"Correlation between expected and predicted: {}\".format(corr))\n",
    "\n",
    "plot_expected_vs_predicted(expected_predicted_df, language_model, language_test)\n",
    "\n",
    "# Get the accuracy\n",
    "accuracy = accuracy_score(y_test_EN, predictions)\n",
    "\n",
    "# Save the coefficients to a CSV file \"C:\\Research\\Results\\{lang}\\coefficients_{date}_corr_{corr}.csv\"\n",
    "save_coefficients(model_EN, X_test_EN, corr, language_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ES predicts EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model = 'ES' # 'EN' or 'ES'\n",
    "language_test = 'EN' # 'EN' or 'ES'\n",
    "\n",
    "# Get the predictions\n",
    "unrounded_predictions, predictions = get_predictions(model_ES, X_test_EN)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_EN, predictions)\n",
    "\n",
    "print_class_accuray(conf_matrix)\n",
    "\n",
    "# Get the accuracy\n",
    "accuracy = accuracy_score(y_test_EN, predictions)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "plot_conf_matrix(conf_matrix, language_model, language_test)\n",
    "\n",
    "expected_predicted_df, corr = compute_correlation(unrounded_predictions, y_test_EN)\n",
    "print(\"Correlation between expected and predicted: {}\".format(corr))\n",
    "\n",
    "plot_expected_vs_predicted(expected_predicted_df, language_model, language_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EN predicts ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model = 'EN' # 'EN' or 'ES'\n",
    "language_test = 'ES' # 'EN' or 'ES'\n",
    "\n",
    "# Get the predictions\n",
    "unrounded_predictions, predictions = get_predictions(model_EN, X_test_ES)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_ES, predictions)\n",
    "\n",
    "print_class_accuray(conf_matrix)\n",
    "\n",
    "# Get the accuracy\n",
    "accuracy = accuracy_score(y_test_ES, predictions)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "plot_conf_matrix(conf_matrix, language_model, language_test)\n",
    "\n",
    "expected_predicted_df, corr = compute_correlation(unrounded_predictions, y_test_ES)\n",
    "print(\"Correlation between expected and predicted: {}\".format(corr))\n",
    "\n",
    "plot_expected_vs_predicted(expected_predicted_df, language_model, language_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model = 'EN' # 'EN' or 'ES'\n",
    "language_test = 'EN' # 'EN' or 'ES'\n",
    "\n",
    "# Instantiate the KNeighborsClassifier object with the number of neighbors you want to consider\n",
    "knn_model_EN = KNeighborsClassifier(n_neighbors=2)\n",
    "\n",
    "# Fit the model to the training data\n",
    "knn_model_EN.fit(X_train_knn_EN, y_train_knn_EN)\n",
    "unrounded_predictions, predictions = get_predictions(knn_model_EN, X_test_knn_EN)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_knn_EN, predictions)\n",
    "\n",
    "print_class_accuray(conf_matrix)\n",
    "\n",
    "# Get the accuracy\n",
    "accuracy = accuracy_score(y_test_knn_EN, predictions)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "plot_conf_matrix(conf_matrix, language_model, language_test)\n",
    "\n",
    "expected_predicted_df, corr = compute_correlation(unrounded_predictions, y_test_knn_EN)\n",
    "print(\"Correlation between expected and predicted: {}\".format(corr))\n",
    "\n",
    "plot_expected_vs_predicted(expected_predicted_df, language_model, language_test)\n",
    "\n",
    "\n",
    "# Save the coefficients to a CSV file \"C:\\Research\\Results\\{lang}\\coefficients_{date}_corr_{corr}.csv\"\n",
    "# save_coefficients(knn_model_EN, X_test_EN, corr, language_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model = 'ES' # 'EN' or 'ES'\n",
    "language_test = 'ES' # 'EN' or 'ES'\n",
    "\n",
    "# Instantiate the KNeighborsClassifier object with the number of neighbors you want to consider\n",
    "knn_model_ES = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "knn_model_ES.fit(X_train_knn_ES, y_train_knn_ES)\n",
    "unrounded_predictions, predictions = get_predictions(knn_model_ES, X_test_knn_ES)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_knn_ES, predictions)\n",
    "\n",
    "print_class_accuray(conf_matrix)\n",
    "\n",
    "# Get the accuracy\n",
    "accuracy = accuracy_score(y_test_knn_ES, predictions)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy*100))\n",
    "\n",
    "plot_conf_matrix(conf_matrix, language_model, language_test)\n",
    "\n",
    "expected_predicted_df, corr = compute_correlation(unrounded_predictions, y_test_knn_ES)\n",
    "print(\"Correlation between expected and predicted: {}\".format(corr))\n",
    "\n",
    "plot_expected_vs_predicted(expected_predicted_df, language_model, language_test)\n",
    "\n",
    "# Save the coefficients to a CSV file \"C:\\Research\\Results\\{lang}\\coefficients_{date}_corr_{corr}.csv\"\n",
    "# save_coefficients(knn_model_ES, X_test_ES, corr, language_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that given a number of miliseconds, prints the time in seconds\n",
    "def print_time(t): # 60300\n",
    "    seconds = t // 1000 # 60\n",
    "    minutes = seconds // 60 # 1\n",
    "    seconds = seconds % 60 # 0\n",
    "    ms = (t % 1000) # 300\n",
    "    # Print the time in seconds and fraction of seconds rounded to 2 decimals\n",
    "    print(\"Time: {}:{}.{}\".format(minutes, seconds, round(ms, 4)))\n",
    "\n",
    "def minutes_to_milliseconds(minutes):\n",
    "    return minutes * 60 * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_num = 5727\n",
    "print_time(window_num * 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = 2\n",
    "lower_bound = 0.4\n",
    "upper_bound = 0.45\n",
    "\n",
    "# Get row of dataframe where column expected has value 'expected' and predicted is between 'lower_bound' and 'upper_bound'\n",
    "ids = [sample_ids[id] for id in (list(expected_predicted_df[(expected_predicted_df['expected'] == expected) & (expected_predicted_df['predicted'] > lower_bound) & (expected_predicted_df['predicted'] < upper_bound)].index))]\n",
    "\n",
    "display(ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
